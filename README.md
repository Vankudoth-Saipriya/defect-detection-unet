# ğŸ­ Multimodal Emotion Detection

A complete AI system for detecting emotions from **text**, **images**, and **audio** using deep learning and natural language processing. Built with PyTorch, HuggingFace Transformers, and deployed via Streamlit.

---

## ğŸ§  Overview

This project combines **three powerful models** into a single system that can analyze human emotion from:
- âœï¸ Written **Text**
- ğŸ“¸ Facial **Image** (via file or webcam)
- ğŸ™ï¸ **Audio** recordings *(coming soon)*

It supports multiple input modes, provides user-friendly interfaces, and is built for deployment.

---

## ğŸ“‚ Project Structure

```
MULTIMODAL_EMOTION_DETECTION/
â”‚
â”œâ”€â”€ app.py                           # Streamlit interface
â”œâ”€â”€ predict_text.py                  # Text emotion inference
â”œâ”€â”€ predict_image_one.py             # Image emotion inference
â”œâ”€â”€ predict_multimodal.py            # Combined model (if needed)
â”‚
â”œâ”€â”€ data/                            # Text datasets
â”‚   â”œâ”€â”€ train.txt / val.txt / test.txt
â”‚   â””â”€â”€ requirements.txt             # Dependencies
â”‚
â”œâ”€â”€ image_data/                      # Image datasets
â”‚   â”œâ”€â”€ train/ val/ test/
â”‚   â””â”€â”€ split_val.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ image_emotion_model/         # Trained CNN model for image
â”‚   â””â”€â”€ text_emotion_model/          # DistilBERT-based model
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer.json, vocab.txt
â”‚       â””â”€â”€ model.safetensors
â”‚
â”œâ”€â”€ text_model_output/               # Trained checkpoints
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ checkpoint-2000/
â”‚   â””â”€â”€ checkpoint-3000/
â”‚
â”œâ”€â”€ text_model/                      # Text model code
â”‚
â”œâ”€â”€ results/                         # Output logs or prediction results
â”‚
â””â”€â”€ venv/                            # Virtual environment
```

---

## ğŸš€ How to Run

### 1. Install dependencies

```bash
pip install -r data/requirements.txt
```

### 2. Run the App

```bash
streamlit run app.py
```

### 3. Interface Options

- ğŸ“· **Image Input**: Upload an image or use webcam
- ğŸ“„ **Text Input**: Type your sentence to detect emotion
- ğŸ”Š **Audio Input**: *(Planned)* Record through mic and predict

---

## ğŸ“Š Models Used

| Modality | Model                      | Library             |
|----------|----------------------------|---------------------|
| Text     | DistilBERT fine-tuned      | HuggingFace Transformers |
| Image    | Custom CNN                 | PyTorch, TorchVision |
| Audio    | *(Planned)* MFCC + CNN     | torchaudio, librosa |

---

## ğŸ§ª Emotion Labels

- **Text**: `joy`, `sadness`, `anger`, `fear`, `surprise`, `love`
- **Image**: `happy`, `sad`, `angry`, `fear`, `disgust`, `surprise`, `neutral`

---

## ğŸ› ï¸ Tech Stack

- Python 3.11
- PyTorch, HuggingFace Transformers
- OpenCV, TorchVision
- Streamlit (App deployment)
- Git, GitHub (Version control)

---

## ğŸ’¡ Future Improvements

- ğŸ™ï¸ Real-time audio emotion detection
- ğŸ”€ Fuse predictions across modalities for higher accuracy
- ğŸŒ Deploy the app to HuggingFace Spaces or Streamlit Cloud
- ğŸ“± Build mobile-responsive UI

---

## ğŸ™‹â€â™€ï¸ Author

**Sai Priya Vankudoth**  
ğŸ“ B.Tech, IIT | Data Science & AI Enthusiast  
ğŸ”— GitHub: [saipriyavankudoth](https://github.com/saipriyavankudoth)  
ğŸ“§ Email: saipriyavankudoth@example.com

---

## ğŸ“„ License

This project is open-source under the [MIT License](LICENSE).
